#!/bin/bash
# ============================================================
# æ¶ˆè´¹æœåŠ¡æ€§èƒ½æµ‹è¯•è„šæœ¬
# ============================================================
# åŠŸèƒ½ï¼šä½¿ç”¨JMeterè¿›è¡Œæ€§èƒ½æµ‹è¯•
# ç›®æ ‡ï¼šTPS â‰¥ 1000, å¹³å‡å“åº”æ—¶é—´ â‰¤ 50ms
# ============================================================

set -e

# é…ç½®å˜é‡
THREADS=100          # å¹¶å‘çº¿ç¨‹æ•°
RAMP_UP=10           # å¯åŠ¨æ—¶é—´ï¼ˆç§’ï¼‰
DURATION=300         # æµ‹è¯•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
TARGET_TPS=1000      # ç›®æ ‡TPS
TARGET_RESPONSE=50   # ç›®æ ‡å“åº”æ—¶é—´ï¼ˆmsï¼‰
TARGET_P95=100       # ç›®æ ‡P95å“åº”æ—¶é—´ï¼ˆmsï¼‰

# æœåŠ¡å™¨é…ç½®
HOST="${HOST:-localhost}"
PORT="${PORT:-8094}"
BASE_URL="http://${HOST}:${PORT}"

# æ—¥å¿—ç›®å½•
LOG_DIR="./performance-test-logs"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
TEST_LOG_DIR="${LOG_DIR}/${TIMESTAMP}"

# é¢œè‰²å®šä¹‰
GREEN='\033[0;32m'
RED='\033[0;31m'
YELLOW='\033[1;33m'
NC='\033[0m'

log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

# ============================================================
# æ­¥éª¤1ï¼šç¯å¢ƒæ£€æŸ¥
# ============================================================
check_environment() {
    log_info "æ­¥éª¤1ï¼šæ£€æŸ¥æµ‹è¯•ç¯å¢ƒ..."

    # æ£€æŸ¥JMeteræ˜¯å¦å®‰è£…
    if ! command -v jmeter &> /dev/null; then
        log_error "JMeteræœªå®‰è£…ï¼è¯·å…ˆå®‰è£…JMeterï¼š"
        echo "  Ubuntu/Debian: sudo apt-get install jmeter"
        echo "  Mac: brew install jmeter"
        echo "  Windows: ä» https://jmeter.apache.org/ ä¸‹è½½"
        exit 1
    fi

    log_info "âœ“ JMeterå·²å®‰è£…"

    # æ£€æŸ¥curlæ˜¯å¦å¯ç”¨
    if ! command -v curl &> /dev/null; then
        log_error "curlæœªå®‰è£…ï¼"
        exit 1
    fi

    log_info "âœ“ curlå·²å®‰è£…"

    # æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
    if ! curl -s "${BASE_URL}/actuator/health" > /dev/null; then
        log_error "æ¶ˆè´¹æœåŠ¡æœªè¿è¡Œæˆ–æ— æ³•è®¿é—®ï¼URL: ${BASE_URL}"
        exit 1
    fi

    log_info "âœ“ æ¶ˆè´¹æœåŠ¡è¿è¡Œæ­£å¸¸: ${BASE_URL}"

    # åˆ›å»ºæ—¥å¿—ç›®å½•
    mkdir -p "$TEST_LOG_DIR"

    log_info "âœ“ æ—¥å¿—ç›®å½•åˆ›å»ºæˆåŠŸ: $TEST_LOG_DIR"
}

# ============================================================
# æ­¥éª¤2ï¼šåˆ›å»ºJMeteræµ‹è¯•è®¡åˆ’
# ============================================================
create_jmeter_test_plan() {
    log_info "æ­¥éª¤2ï¼šåˆ›å»ºJMeteræµ‹è¯•è®¡åˆ’..."

    cat > "${TEST_LOG_DIR}/consume-test.jmx" <<'EOF'
<?xml version="1.0" encoding="UTF-8"?>
<jmeterTestPlan version="1.2" properties="5.0">
  <hashTree>
    <TestPlan guiclass="TestPlanGui" testclass="TestPlan" testname="æ¶ˆè´¹æœåŠ¡æ€§èƒ½æµ‹è¯•">
      <elementProp name="TestPlan.user_defined_variables" elementType="Arguments">
        <collectionProp name="Arguments.arguments">
          <elementProp name="HOST" elementType="Argument">
            <stringProp name="Argument.name">HOST</stringProp>
            <stringProp name="Argument.value">localhost</stringProp>
          </elementProp>
          <elementProp name="PORT" elementType="Argument">
            <stringProp name="Argument.name">PORT</stringProp>
            <stringProp name="Argument.value">8094</stringProp>
          </elementProp>
        </collectionProp>
      </elementProp>
    </TestPlan>
    <hashTree>
      <ThreadGroup guiclass="ThreadGroupGui" testclass="ThreadGroup" testname="æ¶ˆè´¹è¯·æ±‚çº¿ç¨‹ç»„">
        <stringProp name="ThreadGroup.num_threads">100</stringProp>
        <stringProp name="ThreadGroup.ramp_time">10</stringProp>
        <longProp name="ThreadGroup.duration">300</longProp>
        <boolProp name="ThreadGroup.scheduler">true</boolProp>
        <stringProp name="ThreadGroup.scheduler">true</stringProp>
      </ThreadGroup>
      <hashTree>
        <HTTPSamplerProxy guiclass="HttpTestSampleGui" testclass="HTTPSamplerProxy" testname="æ¶ˆè´¹è¯·æ±‚">
          <stringProp name="HTTPSampler.domain">${HOST}</stringProp>
          <stringProp name="HTTPSampler.port">${PORT}</stringProp>
          <stringProp name="HTTPSampler.path">/api/consume/transaction</stringProp>
          <stringProp name="HTTPSampler.method">POST</stringProp>
          <boolProp name="HTTPSampler.use_keepalive">true</boolProp>
        </HTTPSamplerProxy>
        <hashTree/>
        <ResultCollector guiclass="ViewResultsFullVisualizer" testclass="ResultCollector" testname="æŸ¥çœ‹ç»“æœæ ‘">
          <boolProp name="ResultCollector.error_logging">true</boolProp>
          <objProp>
            <name>saveConfig</name>
            <value class="SampleSaveConfiguration">
              <time>true</time>
              <latency>true</latency>
              <success>true</success>
              <responseCode>true</responseCode>
            </value>
          </objProp>
          <stringProp name="filename">${TEST_LOG_DIR}/results.jtl</stringProp>
        </ResultCollector>
        <hashTree/>
      </hashTree>
    </hashTree>
  </hashTree>
</jmeterTestPlan>
EOF

    log_info "âœ“ JMeteræµ‹è¯•è®¡åˆ’åˆ›å»ºæˆåŠŸ"
}

# ============================================================
# æ­¥éª¤3ï¼šæ‰§è¡Œæ€§èƒ½æµ‹è¯•
# ============================================================
run_performance_test() {
    log_info "æ­¥éª¤3ï¼šæ‰§è¡Œæ€§èƒ½æµ‹è¯•..."
    log_info "æµ‹è¯•å‚æ•°ï¼š"
    log_info "  å¹¶å‘çº¿ç¨‹æ•°: $THREADS"
    log_info "  å¯åŠ¨æ—¶é—´: ${RAMP_UP}ç§’"
    log_info "  æµ‹è¯•æ—¶é•¿: ${DURATION}ç§’"
    log_info "  ç›®æ ‡TPS: $TARGET_TPS"
    log_info "  ç›®æ ‡å“åº”æ—¶é—´: ${TARGET_RESPONSE}ms"

    # æ‰§è¡ŒJMeteræµ‹è¯•
    jmeter -n -t "${TEST_LOG_DIR}/consume-test.jmx" \
           -l "${TEST_LOG_DIR}/test-results.jtl" \
           -e -o "${TEST_LOG_DIR}/html-report" \
           -JHOST=$HOST -JPORT=$PORT

    log_info "âœ“ æ€§èƒ½æµ‹è¯•å®Œæˆ"
}

# ============================================================
# æ­¥éª¤4ï¼šåˆ†ææµ‹è¯•ç»“æœ
# ============================================================
analyze_results() {
    log_info "æ­¥éª¤4ï¼šåˆ†ææµ‹è¯•ç»“æœ..."

    # æ£€æŸ¥æµ‹è¯•ç»“æœæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if [ ! -f "${TEST_LOG_DIR}/test-results.jtl" ]; then
        log_error "æµ‹è¯•ç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼"
        exit 1
    fi

    # ä½¿ç”¨Pythonè„šæœ¬è§£æJTLæ–‡ä»¶
    python3 <<PYTHON_SCRIPT
import xml.etree.ElementTree as ET
import sys

# è§£æJTLæ–‡ä»¶
tree = ET.parse('${TEST_LOG_DIR}/test-results.jtl')
root = tree.getroot()

# ç»Ÿè®¡æŒ‡æ ‡
total_samples = 0
successful_samples = 0
failed_samples = 0
total_time = 0
min_time = float('inf')
max_time = 0

# æ”¶é›†æ‰€æœ‰å“åº”æ—¶é—´
response_times = []

for sample in root.findall('httpSample'):
    total_samples += 1
    success = sample.get('s')
    time = int(sample.get('t'))

    response_times.append(time)
    total_time += time

    if time < min_time:
        min_time = time
    if time > max_time:
        max_time = time

    if success == 'true':
        successful_samples += 1
    else:
        failed_samples += 1

# è®¡ç®—æŒ‡æ ‡
if total_samples > 0:
    avg_time = total_time / total_samples
    success_rate = (successful_samples / total_samples) * 100
    tps = total_samples / ${DURATION}

    # è®¡ç®—P95å“åº”æ—¶é—´
    response_times.sort()
    p95_index = int(len(response_times) * 0.95)
    p95_time = response_times[p95_index] if p95_index < len(response_times) else max_time

    # è¾“å‡ºç»“æœ
    print("\\n" + "="*60)
    print("æ€§èƒ½æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*60)
    print(f"æ€»è¯·æ±‚æ•°: {total_samples}")
    print(f"æˆåŠŸè¯·æ±‚æ•°: {successful_samples}")
    print(f"å¤±è´¥è¯·æ±‚æ•°: {failed_samples}")
    print(f"æˆåŠŸç‡: {success_rate:.2f}%")
    print(f"TPS: {tps:.2f}")
    print(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ms")
    print(f"æœ€å°å“åº”æ—¶é—´: {min_time}ms")
    print(f"æœ€å¤§å“åº”æ—¶é—´: {max_time}ms")
    print(f"P95å“åº”æ—¶é—´: {p95_time}ms")
    print("="*60)

    # éªŒè¯æ˜¯å¦è¾¾æ ‡
    print("\\néªŒè¯ç»“æœï¼š")
    tps_pass = tps >= ${TARGET_TPS}
    response_pass = avg_time <= ${TARGET_RESPONSE}
    p95_pass = p95_time <= ${TARGET_P95}

    if tps_pass:
        print(f"âœ“ TPSè¾¾æ ‡: {tps:.2f} >= ${TARGET_TPS}")
    else:
        print(f"âœ— TPSæœªè¾¾æ ‡: {tps:.2f} < ${TARGET_TPS}")

    if response_pass:
        print(f"âœ“ å¹³å‡å“åº”æ—¶é—´è¾¾æ ‡: {avg_time:.2f}ms <= ${TARGET_RESPONSE}ms")
    else:
        print(f"âœ— å¹³å‡å“åº”æ—¶é—´æœªè¾¾æ ‡: {avg_time:.2f}ms > ${TARGET_RESPONSE}ms")

    if p95_pass:
        print(f"âœ“ P95å“åº”æ—¶é—´è¾¾æ ‡: {p95_time}ms <= ${TARGET_P95}ms")
    else:
        print(f"âœ— P95å“åº”æ—¶é—´æœªè¾¾æ ‡: {p95_time}ms > ${TARGET_P95}ms")

    if tps_pass and response_pass and p95_pass:
        print("\\nğŸ‰ æ€§èƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        sys.exit(0)
    else:
        print("\\nâš ï¸ æ€§èƒ½æµ‹è¯•æœªå…¨éƒ¨é€šè¿‡ï¼Œè¯·ä¼˜åŒ–ï¼")
        sys.exit(1)

PYTHON_SCRIPT

    PYTHON_EXIT_CODE=$?

    if [ $PYTHON_EXIT_CODE -eq 0 ]; then
        log_info "âœ“ æ€§èƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼"
        return 0
    else
        log_error "æ€§èƒ½æµ‹è¯•æœªå…¨éƒ¨é€šè¿‡ï¼"
        return 1
    fi
}

# ============================================================
# ä¸»æµç¨‹
# ============================================================
main() {
    log_info "========================================"
    log_info "æ¶ˆè´¹æœåŠ¡æ€§èƒ½æµ‹è¯•"
    log_info "========================================"
    log_info "æœåŠ¡å™¨: ${BASE_URL}"
    log_info "æ—¥å¿—ç›®å½•: ${TEST_LOG_DIR}"
    log_info "========================================"
    echo ""

    # æ‰§è¡Œæµ‹è¯•
    check_environment
    create_jmeter_test_plan
    run_performance_test
    analyze_results

    log_info ""
    log_info "========================================"
    log_info "æ€§èƒ½æµ‹è¯•å®Œæˆï¼"
    log_info "è¯¦ç»†æŠ¥å‘Š: ${TEST_LOG_DIR}/html-report/index.html"
    log_info "========================================"
}

# æ‰§è¡Œä¸»æµç¨‹
main
